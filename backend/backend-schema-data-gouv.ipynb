{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from git import Repo, Git\n",
    "import os\n",
    "import shutil\n",
    "from table_schema_to_markdown import convert_source\n",
    "import frictionless\n",
    "import glob\n",
    "import json\n",
    "import jsonschema\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import codecs\n",
    "import requests\n",
    "from urllib import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_FOLDER = './cache'\n",
    "DATA_FOLDER1 = './data'\n",
    "DATA_FOLDER2 = './data2'\n",
    "ERRORS_REPORT = []\n",
    "SCHEMA_INFOS = {}\n",
    "SCHEMA_CATALOG = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze validity of every release of every schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading yaml file containing all schemas that we want to display in schema.data.gouv.fr\n",
    "with open(\"repertoires.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_create_folder(folder):\n",
    "    \"\"\"Remove local folder if exist and (re)create it\"\"\"\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "    os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consolidated_version(tag):\n",
    "    \"\"\"Analyze tag from a code source release, cast it to acceptable semver version X.X.X\"\"\"\n",
    "    valid_version = True\n",
    "    # Removing 'v' or 'V' from tag\n",
    "    version_items = str(tag).replace('v','').replace('V','').split('.')\n",
    "    # Add a patch number if only 2 items\n",
    "    if len(version_items) == 2: version_items.append('0')\n",
    "    # If more than 3, do not accept tag\n",
    "    if len(version_items) > 3: valid_version = False\n",
    "    # Verify if all items are digits\n",
    "    for v in version_items:\n",
    "        if not v.isdigit():\n",
    "            valid_version = False\n",
    "    # Return semver version and validity of it\n",
    "    return '.'.join(version_items), valid_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_errors(repertoire_slug, version, reason):\n",
    "    \"\"\"Create dictionnary that will populate ERRORS_REPORT object\"\"\"\n",
    "    mydict = {}\n",
    "    mydict['schema'] = repertoire_slug\n",
    "    mydict['version'] = version\n",
    "    mydict['type'] = reason\n",
    "    ERRORS_REPORT.append(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_schema(repertoire_slug, conf, schema_type):\n",
    "    \"\"\"Check validity of schema and all of its releases\"\"\"\n",
    "    # schema_name in schema.data.gouv.fr is referenced by group and repo name in Git. Ex : etalab / schema-irve\n",
    "    schema_name = '/'.join(conf['url'].split('.git')[0].split('/')[-2:])\n",
    "    # define source folder and create it\n",
    "    # source folder will help us to checkout to every release and analyze source code for each one\n",
    "    src_folder = CACHE_FOLDER + '/' + schema_name + '/'\n",
    "    os.makedirs(src_folder)\n",
    "    # clone repo in source folder\n",
    "    Repo.clone_from(conf['url'], src_folder)\n",
    "    repo = Repo(src_folder)\n",
    "    # get tags of repo\n",
    "    tags = sorted(repo.tags, key=lambda t: t.commit.committed_datetime)\n",
    "    \n",
    "    list_schemas = {}\n",
    "    \n",
    "    # Defining SCHEMA_INFOS object for website use\n",
    "    SCHEMA_INFOS[schema_name] = {}\n",
    "    SCHEMA_INFOS[schema_name]['homepage'] = conf['url']\n",
    "    SCHEMA_INFOS[schema_name]['external_doc'] = conf['external_doc'] if 'external_doc' in conf else None\n",
    "    SCHEMA_INFOS[schema_name]['external_tool'] = conf['external_tool'] if 'external_tool' in conf else None\n",
    "    SCHEMA_INFOS[schema_name]['type'] = conf['type']\n",
    "    SCHEMA_INFOS[schema_name]['email'] = conf['email']\n",
    "    SCHEMA_INFOS[schema_name]['versions'] = {}\n",
    "    \n",
    "    # for every tags\n",
    "    for t in tags:\n",
    "        # get semver version and validity of it\n",
    "        version, valid_version = get_consolidated_version(t)\n",
    "        # if semver ok\n",
    "        if(valid_version):\n",
    "            # define destination folder and create it\n",
    "            # destination folder will store pertinents files for website for each version of each schema\n",
    "            dest_folder = DATA_FOLDER1 + '/' + schema_name + '/' + version + '/'\n",
    "            os.makedirs(dest_folder)\n",
    "            # checkout to current version\n",
    "            g = Git(src_folder)\n",
    "            g.checkout(str(t))\n",
    "            \n",
    "            conf_schema = None\n",
    "            # Managing validation differently for each type of schema\n",
    "            # tableschema will use frictionless package\n",
    "            # jsonschema will use jsonschema package\n",
    "            # other will only check if schema.yml file is present and contain correct information\n",
    "            if(schema_type == 'tableschema'):\n",
    "                list_schemas = manage_tableschema(src_folder, dest_folder, list_schemas, version, schema_name)\n",
    "            if(schema_type == 'jsonschema'):\n",
    "                list_schemas, conf_schema = manage_jsonschema(src_folder, dest_folder, list_schemas, version, schema_name)\n",
    "            if(schema_type == 'other'):\n",
    "                list_schemas, conf_schema = manage_other(src_folder, dest_folder, list_schemas, version, schema_name)\n",
    "    \n",
    "    # Find latest valid version and create a specific folder 'latest' copying files in it (for website use)\n",
    "    latest_folder, list_schemas, schema_file = manage_latest_folder(conf, list_schemas, schema_name)\n",
    "    # Complete catalog with all relevant information of schema in it\n",
    "    schema_to_add_to_catalog = generate_catalog_object(latest_folder, list_schemas, schema_file, schema_type, schema_name, conf_schema)\n",
    "    return schema_to_add_to_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_tableschema(src_folder, dest_folder, list_schemas, version, schema_name):\n",
    "    \"\"\"Check validity of a schema release from tableschema type\"\"\"\n",
    "    # Verify that a file schema.json is present\n",
    "    if(os.path.isfile(src_folder + 'schema.json')):\n",
    "        # Validate it with frictionless package\n",
    "        frictionless_report = frictionless.validate(src_folder + 'schema.json')\n",
    "        # If schema release is valid, then\n",
    "        if(frictionless_report['valid'] == True):\n",
    "            list_schemas[version] = 'schema.json'\n",
    "            # We complete info of version\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version] = {}\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version]['pages'] = []\n",
    "            # We check for list of normalized files if it is present in source code, if so, we copy paste them into dest folder\n",
    "            for f in ['schema.json', 'README.md', 'SEE_ALSO.md', 'CHANGELOG.md', 'CONTEXT.md']:\n",
    "                if(os.path.isfile(src_folder + f)):\n",
    "                    shutil.copyfile(src_folder + f,dest_folder + f)\n",
    "                    # if it is a markdown file, we will read them as page in website\n",
    "                    if(f[-3:] == '.md'):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append(f)\n",
    "                    # if it is the schema, we indicate it as it in object\n",
    "                    if(f == 'schema.json'):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['schema_url'] = '/' + schema_name + '/' + version + '/' + 'schema.json'\n",
    "            # Create documentation file and save it\n",
    "            with open(dest_folder + 'documentation.md', \"w\") as out:\n",
    "                try:\n",
    "                    # From schema.json, we use tableschema_to_markdown package to convert it in a\n",
    "                    # readable mardown file that will be use for documentation\n",
    "                    convert_source(dest_folder + 'schema.json', out, 'page',[])\n",
    "                    SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append('documentation.md')\n",
    "                except:\n",
    "                    # if conversion is on error, we add it to ERRORS_REPORT\n",
    "                    manage_errors(repertoire_slug, version, 'convert to markdown')\n",
    "        # If schema release is not valid, we remove it from DATA_FOLDER1\n",
    "        else:\n",
    "            manage_errors(repertoire_slug, version, 'tableschema validation')\n",
    "            shutil.rmtree(dest_folder)\n",
    "    # If there is no schema.json, schema release is not valid, we remove it from DATA_FOLDER1\n",
    "    else:\n",
    "        manage_errors(repertoire_slug, version, 'missing schema.json')\n",
    "        shutil.rmtree(dest_folder)\n",
    "    \n",
    "    return list_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_jsonschema(src_folder, dest_folder, list_schemas, version, schema_name):\n",
    "    \"\"\"Check validity of a schema release from jsonschema type\"\"\"\n",
    "    conf_schema = None\n",
    "    # Verify that a file schemas.yml is present\n",
    "    # This file will indicate title, description of jsonschema (it is a prerequisite asked by schema.data.gouv.fr)\n",
    "    # This file will also indicate which file store the jsonschema schema\n",
    "    if(os.path.isfile(src_folder + 'schemas.yml')):\n",
    "        try:\n",
    "            with open(src_folder + 'schemas.yml', \"r\") as f:\n",
    "                conf_schema = yaml.safe_load(f)\n",
    "                if('schemas' in conf_schema):\n",
    "                    s = conf_schema['schemas'][0]\n",
    "                    # Verify if jsonschema file indicate in schemas.yml is present, then load it\n",
    "                    if(os.path.isfile(src_folder + s['path'])):\n",
    "                        with open(src_folder + s['path'], \"r\") as f:\n",
    "                            schema_data = json.load(f)\n",
    "                        # Validate schema with jsonschema package\n",
    "                        jsonschema.validators.validator_for(schema_data).check_schema(schema_data)\n",
    "                        list_schemas[version] = s['path']\n",
    "                        # We complete info of version\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version] = {}\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['pages'] = []\n",
    "                        # We check for list of normalized files if it is present in source code, if so, we copy paste them into dest folder\n",
    "                        for f in ['README.md', 'SEE_ALSO.md', 'CHANGELOG.md', 'CONTEXT.md', s['path']]:\n",
    "                            if(os.path.isfile(src_folder + f)):\n",
    "                                shutil.copyfile(src_folder + f,dest_folder + f)\n",
    "                            # if it is a markdown file, we will read them as page in website\n",
    "                            if(f[-3:] == '.md'):\n",
    "                                SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append(f)\n",
    "                            # if it is the schema, we indicate it as it in object\n",
    "                            if(f == s['path']):\n",
    "                                SCHEMA_INFOS[schema_name]['versions'][version]['schema_url'] = '/' + schema_name + '/' + version + '/' + s['path']\n",
    "        # If schema release is not valid, we remove it from DATA_FOLDER1\n",
    "        except:\n",
    "            manage_errors(repertoire_slug, version, 'jsonschema validation')\n",
    "            shutil.rmtree(dest_folder)\n",
    "    # If there is no schemas.yml, schema release is not valid, we remove it from DATA_FOLDER1    \n",
    "    else:\n",
    "        manage_errors(repertoire_slug, version, 'missing schemas.yml')\n",
    "        shutil.rmtree(dest_folder)\n",
    "    \n",
    "    return list_schemas, conf_schema\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_other(src_folder, dest_folder, list_schemas, version, schema_name):\n",
    "    \"\"\"Check validity of a schema release from other type\"\"\"\n",
    "    conf_schema = None\n",
    "    # Verify that a file schema.yml is present\n",
    "    # This file will indicate title, description of schema (it is a prerequisite asked by schema.data.gouv.fr)\n",
    "    if(os.path.isfile(src_folder + 'schema.yml')):\n",
    "        try:\n",
    "            with open(src_folder + 'schema.yml', \"r\") as f:\n",
    "                conf_schema = yaml.safe_load(f)\n",
    "            list_schemas[version] = 'schema.yml'\n",
    "            # We complete info of version\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version] = {}\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version]['pages'] = []\n",
    "            # We check for list of normalized files if it is present in source code, if so, we copy paste them into dest folder\n",
    "            for f in ['README.md', 'SEE_ALSO.md', 'CHANGELOG.md', 'CONTEXT.md', 'schema.yml']:\n",
    "                if(os.path.isfile(src_folder + f)):\n",
    "                    shutil.copyfile(src_folder + f,dest_folder + f)\n",
    "                    # if it is a markdown file, we will read them as page in website\n",
    "                    if(f[-3:] == '.md'):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append(f)\n",
    "                    # if it is the schema, we indicate it as it in object\n",
    "                    if(f == 'schema.yml'):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['schema_url'] = '/' + schema_name + '/' + version + '/' + 'schema.yml'\n",
    "        # If schema release is not valid, we remove it from DATA_FOLDER1\n",
    "        except:\n",
    "            manage_errors(repertoire_slug, version, 'validation of type other')\n",
    "            shutil.rmtree(dest_folder)\n",
    "    # If there is no schema.yml, schema release is not valid, we remove it from DATA_FOLDER1    \n",
    "    else:\n",
    "        manage_errors(repertoire_slug, version, 'missing schema.yml')\n",
    "        shutil.rmtree(dest_folder)\n",
    "    \n",
    "    return list_schemas, conf_schema\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_latest_folder(conf, list_schemas, schema_name):\n",
    "    \"\"\"Create latest folder containing all files from latest valid version of a schema\"\"\"\n",
    "    # Get all valid version from a schema by analyzing folders, then sort them to get latest valid version and related folder\n",
    "    subfolders = [ f.name for f in os.scandir(DATA_FOLDER1 + '/' + schema_name + '/') if f.is_dir() ]\n",
    "    subfolders.sort()\n",
    "    latest_version_folder = DATA_FOLDER1 + '/' + schema_name + '/' + subfolders[-1] + '/'\n",
    "    # Determine latest folder path then mkdir it\n",
    "    latest_folder = DATA_FOLDER1 + '/' + schema_name + '/latest/'\n",
    "    os.makedirs(latest_folder)\n",
    "    # For every file in latest valid version folder, copy them into \n",
    "    for f in glob.glob(latest_version_folder + '*'):\n",
    "        shutil.copyfile(f,latest_folder + f.split('/')[-1])\n",
    "    # For website need, copy paste latest README into root of schema folder\n",
    "    shutil.copyfile(latest_version_folder+'README.md','/'.join(latest_version_folder.split('/')[:-2])+'/README.md')\n",
    "    # Indicate in schema_info object name of latest schema\n",
    "    SCHEMA_INFOS[schema_name]['latest'] = subfolders[-1]\n",
    "    return latest_folder, list_schemas, list_schemas[subfolders[-1]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_catalog_object(latest_folder, list_schemas, schema_file, schema_type, schema_name, obj_info=None):\n",
    "    \"\"\"Generate dictionnary containing all relevant information for catalog\"\"\"\n",
    "    # If tableschema, relevant information are directly into schema.json, \n",
    "    # if not, relevant info are in yaml files with are stored in obj_info variable\n",
    "    if(schema_type == 'tableschema'):\n",
    "        with open(latest_folder + schema_file, \"r\") as f:\n",
    "            schema = json.load(f)\n",
    "    else:\n",
    "        schema = obj_info\n",
    "    # Complete dictionnary with relevant info needed in catalog\n",
    "    mydict = {}\n",
    "    mydict['name'] = schema_name\n",
    "    mydict['title'] = schema['title']\n",
    "    mydict['description'] = schema['description']\n",
    "    mydict['schema_url'] = 'https://schema.data.gouv.fr/schemas/' + schema_name + '/latest/' + schema_file\n",
    "    mydict['schema_type'] = schema_type\n",
    "    mydict['contact'] = conf['email']\n",
    "    mydict['examples'] = schema['resources'] if 'resources' in schema else []\n",
    "    mydict['versions'] = []\n",
    "    for sf in list_schemas:\n",
    "        mydict2 = {}\n",
    "        mydict2['version_name'] = sf\n",
    "        mydict2['schema_url'] = 'https://schema.data.gouv.fr/schemas/' + schema_name + '/' + sf + '/' + list_schemas[sf]\n",
    "        mydict['versions'].append(mydict2)\n",
    "    # These three following property are not in catalog spec \n",
    "    mydict['external_doc'] = conf['external_doc'] if 'external_doc' in conf else None\n",
    "    mydict['external_tool'] = conf['external_tool'] if 'external_tool' in conf else None\n",
    "    mydict['homepage'] = conf['url']\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- irve processed\n",
      "--- format-commande-publique processed\n",
      "--- decp-dpa processed\n",
      "--- catalogue processed\n",
      "--- deliberations processed\n",
      "--- equipements processed\n",
      "--- subventions processed\n",
      "--- covoiturage processed\n",
      "--- infos-travaux processed\n",
      "--- stationnement processed\n",
      "--- budget processed\n",
      "--- dae processed\n",
      "--- prenoms processed\n",
      "--- arbres_urbains processed\n",
      "--- amenagements-cyclables processed\n",
      "--- stationnement-cyclable processed\n",
      "--- inclusion-numerique processed\n",
      "--- bal processed\n",
      "--- hautes-remunerations processed\n",
      "--- menus-restauration processed\n",
      "--- plats-restauration processed\n",
      "--- accessibilite-erp processed\n",
      "--- sdirve processed\n",
      "--- registre-archive processed\n",
      "--- datatourisme processed\n",
      "--- schema-zfe processed\n",
      "--- arrete-circulation-marchandise processed\n",
      "--- stations-taxi processed\n",
      "--- vehicules-faibles-emissions-renouvellement-parc processed\n",
      "--- point-eau-incendie processed\n",
      "--- itineraire-randonnee processed\n",
      "--- emprise-stationnement-voirie processed\n",
      "--- passage-a-niveau processed\n",
      "--- mobilite-site processed\n",
      "--- mobilite-channel processed\n",
      "--- mobilite-measure processed\n"
     ]
    }
   ],
   "source": [
    "# Clean and (re)create CACHE AND DATA FOLDER\n",
    "clean_and_create_folder(CACHE_FOLDER)\n",
    "clean_and_create_folder(DATA_FOLDER1)\n",
    "\n",
    "# Initiate Catalog\n",
    "SCHEMA_CATALOG['$schema'] = 'https://opendataschema.frama.io/catalog/schema-catalog.json'\n",
    "SCHEMA_CATALOG['version'] = 1\n",
    "SCHEMA_CATALOG['schemas'] = []\n",
    "\n",
    "# For every schema in repertoires.yml, check it\n",
    "for repertoire_slug, conf in config.items():\n",
    "    schema_to_add_to_catalog = check_schema(repertoire_slug, conf, conf['type'])\n",
    "    # Append info to SCHEMA_CATALOG\n",
    "    SCHEMA_CATALOG['schemas'].append(schema_to_add_to_catalog)\n",
    "    print('--- {} processed'.format(repertoire_slug))\n",
    "\n",
    "# Save catalog to schemas.json file\n",
    "with open(DATA_FOLDER1 + '/schemas.json', 'w') as fp:\n",
    "    json.dump(SCHEMA_CATALOG, fp)\n",
    "    \n",
    "# Save schemas_infos to schema-infos.json file\n",
    "with open(DATA_FOLDER1 + '/schema-infos.json', 'w') as fp:\n",
    "    json.dump(SCHEMA_INFOS, fp)\n",
    "    \n",
    "# Save errors to errors.json file\n",
    "with open(DATA_FOLDER1 + '/errors.json', 'w') as fp:\n",
    "    json.dump(SCHEMA_INFOS, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema relative files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_md_links(md):\n",
    "    \"\"\"Returns dict of links in markdown:\n",
    "    'regular': [foo](some.url)\n",
    "    'footnotes': [foo][3]\n",
    "    \n",
    "    [3]: some.url\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/a/30738268/2755116\n",
    "    INLINE_LINK_RE = re.compile(r'\\[([^\\]]+)\\]\\(([^)]+)\\)')\n",
    "    FOOTNOTE_LINK_TEXT_RE = re.compile(r'\\[([^\\]]+)\\]\\[(\\d+)\\]')\n",
    "    FOOTNOTE_LINK_URL_RE = re.compile(r'\\[(\\d+)\\]:\\s+(\\S+)')\n",
    "\n",
    "    links = list(INLINE_LINK_RE.findall(md))\n",
    "    footnote_links = dict(FOOTNOTE_LINK_TEXT_RE.findall(md))\n",
    "    footnote_urls = dict(FOOTNOTE_LINK_URL_RE.findall(md))\n",
    "\n",
    "    footnotes_linking = []\n",
    "        \n",
    "    for key in footnote_links.keys():\n",
    "        footnotes_linking.append((footnote_links[key], footnote_urls[footnote_links[key]]))\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLinksDocumentation(dest_folder):\n",
    "    \"\"\"Custom cleaning for links in markdown\"\"\"\n",
    "    # For every documentation.md file, do some custom cleaning for links\n",
    "    file = codecs.open(dest_folder + 'documentation.md', \"r\", \"utf-8\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    # Find all links in file\n",
    "    links = find_md_links(data)\n",
    "    # For each one, lower string then manage space ; _ ; --- and replace them by -\n",
    "    for (name, link) in links:\n",
    "        if(link.startswith('#')):\n",
    "            newlink = link.lower()\n",
    "            newlink = newlink.replace(' ','-')\n",
    "            newlink = newlink.replace('_','-')\n",
    "            newlink = unidecode(newlink, \"utf-8\")\n",
    "            newlink = newlink.replace('---','-')\n",
    "            data = data.replace(link,newlink)\n",
    "    # Save modifications\n",
    "    with open(dest_folder + 'documentation.md', 'w', encoding='utf-8') as fin:\n",
    "        fin.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFrontToMarkdown(dest_folder, f):\n",
    "    \"\"\"Custom add to every markdown files\"\"\"\n",
    "    # for every markdown files\n",
    "    file = codecs.open(dest_folder + f, \"r\", \"utf-8\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    # Add specific tag for website interpretation\n",
    "    data = \"<MenuSchema />\\n\\n\"+data\n",
    "    # Exception scdl Budget not well interpreted by vuepress\n",
    "    data = data.replace('<DocumentBudgetaire>', 'DocumentBudgetaire')\n",
    "    # Save modification\n",
    "    with open(dest_folder + f, 'w', encoding='utf-8') as fin:\n",
    "        fin.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(dirName):\n",
    "    \"\"\"Get list off all files in a specific folder\"\"\"\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "    return allFiles\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data2'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of all files in DATA_FOLDER\n",
    "files = []\n",
    "files = getListOfFiles(DATA_FOLDER1)\n",
    "# Create list of file that we do not want to copy paste\n",
    "avoid_files = [DATA_FOLDER1 + '/' + s['name'] + '/README.md' for s in SCHEMA_CATALOG['schemas']]\n",
    "# for every file\n",
    "for f in files:\n",
    "    # if it is a markdown, add custom front to content\n",
    "    if(f[-3:] == '.md'):\n",
    "        addFrontToMarkdown('/'.join(f.split('/')[:-1])+'/', f.split('/')[-1])\n",
    "    # if it is the documentation file, clean links on it\n",
    "    if(f.split('/')[-1] == 'documentation.md'):\n",
    "        cleanLinksDocumentation('/'.join(f.split('/')[:-1])+'/')\n",
    "    # if it is a README file (except if on avoid_list), copy paste it to root folder of schema (for website use)\n",
    "    # That will create README file with name X.X.X.md (X.X.X corresponding to a specific version)\n",
    "    if(f.split('/')[-1] == 'README.md'):\n",
    "        if f not in avoid_files:\n",
    "            shutil.copyfile(f, f.replace('/README.md','.md'))\n",
    "\n",
    "# Clean and (re)create DATA_FOLDER2, then copy paste all DATA_FOLDER1 into DATA_FOLDER2\n",
    "# DATA_FOLDER1 will be use to contain all markdown files\n",
    "# DATA_FOLDER2 will be use to contain all yaml and json files \n",
    "# This is needed for vuepress that need to store page in one place and 'resources' in another\n",
    "if os.path.exists(DATA_FOLDER2):\n",
    "    shutil.rmtree(DATA_FOLDER2)\n",
    "shutil.copytree(DATA_FOLDER1, DATA_FOLDER2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contributors(url):\n",
    "    \"\"\"Get list off all contributors of a specific git repo\"\"\"\n",
    "    parse_url = parse.urlsplit(url)\n",
    "    # if github, use github api\n",
    "    if('github.com' in parse_url.netloc):\n",
    "        api_url =  parse_url.scheme+'://api.github.com/repos/'+parse_url.path[1:].replace('.git','')+'/contributors'\n",
    "    # else, use gitlab api\n",
    "    else:\n",
    "        api_url =  parse_url.scheme+'://'+parse_url.netloc+'/api/v4/projects/'+parse_url.path[1:].replace('/','%2F').replace('.git','')+'/repository/contributors'\n",
    "    try:\n",
    "        r = requests.get(api_url)\n",
    "        return len(r.json())\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For every issue, request them by label schema status (en investigation or en construction)\n",
    "mydict = {}\n",
    "labels = ['construction', 'investigation']\n",
    "# For each label, get relevant info via github api of schema.data.gouv.fr repo\n",
    "for l in labels:\n",
    "    r = requests.get('https://api.github.com/repos/etalab/schema.data.gouv.fr/issues?q=is%3Aopen+is%3Aissue&labels=Sch%C3%A9ma%20en%20'+l)\n",
    "    mydict[l] = []\n",
    "    for issue in r.json():\n",
    "        mydict2 = {}\n",
    "        mydict2['created_at'] = issue['created_at']\n",
    "        mydict2['labels'] = [l]\n",
    "        mydict2['nb_comments'] = issue['comments']\n",
    "        mydict2['title'] = issue['title']\n",
    "        mydict2['url'] = issue['html_url']\n",
    "        mydict[l].append(mydict2)\n",
    "\n",
    "# Find number of current issue in schema.data.gouv.fr repo\n",
    "r = requests.get('https://api.github.com/repos/etalab/schema.data.gouv.fr/issues?q=is%3Aopen+is%3Aissue')\n",
    "mydict['nb_issues'] = len(r.json())\n",
    "\n",
    "# for every schema, find relevant info in data.gouv.fr API\n",
    "mydict['references'] = {}\n",
    "for s in SCHEMA_CATALOG['schemas']:\n",
    "    r = requests.get('https://www.data.gouv.fr/api/1/datasets/?schema='+s['name'])\n",
    "    mydict['references'][s['name']] = {}\n",
    "    mydict['references'][s['name']]['dgv_resources'] = r.json()['total']\n",
    "    mydict['references'][s['name']]['title'] = s['title']\n",
    "    mydict['references'][s['name']]['contributors'] = get_contributors(s['homepage'])\n",
    "\n",
    "# Save stats infos to stats.json file\n",
    "with open(DATA_FOLDER2 + '/stats.json', 'w') as fp:\n",
    "    json.dump(mydict, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prod folders for website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_files_extension(folder, extension):\n",
    "    \"\"\"Remove all file of a specific extension in a folder\"\"\"\n",
    "    files = []\n",
    "    files = getListOfFiles(folder)\n",
    "    for f in files:\n",
    "        if f[-1*len(extension):] == extension:\n",
    "            os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../site/site/.vuepress/public/schema-infos.json'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all markdown from DATA_FOLDER1 and all json, yaml and yml file of DATA_FOLDER2\n",
    "remove_all_files_extension(DATA_FOLDER2, '.md')\n",
    "remove_all_files_extension(DATA_FOLDER1, '.json')\n",
    "remove_all_files_extension(DATA_FOLDER1, '.yml')\n",
    "remove_all_files_extension(DATA_FOLDER1, '.yaml')\n",
    "\n",
    "# Remove actual currend prod folder in website\n",
    "folder_to_remove = glob.glob('../site/site/*/')\n",
    "folder_to_remove.append('../site/site/.vuepress/public/schemas')\n",
    "for folder in folder_to_remove:\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "\n",
    "# Copy all markdown in site folder, all resources in site/.vuepress/public/schemas/folder\n",
    "shutil.copytree(DATA_FOLDER1, '../site/site/', dirs_exist_ok=True)\n",
    "shutil.copytree(DATA_FOLDER2, '../site/site/.vuepress/public/schemas/', dirs_exist_ok=True)\n",
    "\n",
    "# Copy three main json files into another public folder\n",
    "shutil.copyfile('../site/site/.vuepress/public/schemas/schemas.json','../site/site/.vuepress/public/schemas.json')\n",
    "shutil.copyfile('../site/site/.vuepress/public/schemas/stats.json','../site/site/.vuepress/public/stats.json')\n",
    "shutil.copyfile('../site/site/.vuepress/public/schemas/schema-infos.json','../site/site/.vuepress/public/schema-infos.json')\n",
    "shutil.copyfile('../site/site/.vuepress/public/schemas/errors.json','../site/site/.vuepress/public/errors.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6224c542726492982829fea3fc5066cee43ae47aa086ecd8a28c7a969dcd0d45"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
